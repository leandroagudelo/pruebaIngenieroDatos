# Pipeline CSVâ†’Postgres RAWâ†’SILVERâ†’GOLD

Este proyecto crea un laboratorio de ingenierÃ­a de datos ejecutable con **Docker Compose** que implementa una arquitectura **RAW â†’ SILVER â†’ GOLD** alimentada desde archivos **CSV** y almacenada en **Postgres 16**.  
Incluye micro-lotes configurables, bitÃ¡cora de cargas y un **reporte HTML** para auditorÃ­a.

---

## ğŸ§© Servicios

| Servicio   | PropÃ³sito                                                                             | Imagen                     |
| ---------- | ------------------------------------------------------------------------------------- | -------------------------- |
| `postgres` | Base de datos Postgres 16 con los esquemas `raw`, `silver` y `gold`.                  | `postgres:16`              |
| `app`      | CLI Python 3.11 para orquestar el pipeline (`app/pipeline_pg.py`).                    | `python:3.11-slim`         |
| `jupyter`  | Entorno interactivo `jupyter/minimal-notebook` (abre `notebooks/0_START_HERE.ipynb`). | `jupyter/minimal-notebook` |
| `pgadmin`  | Consola pgAdmin con servidor preconfigurado (`local-postgres`) (opcional).            | `dpage/pgadmin4`           |

> Montajes clave del `app`:  
> `./app â†’ /opt/pipeline` y `./data â†’ /opt/pipeline/data`.  
> Por defecto, los CSV se buscan en **`/opt/pipeline/data/raw`**.

---

## âœ… Requisitos previos

- Docker + Docker Compose (plugin `docker compose`)
- `make`

---

## âš¡ EjecuciÃ³n â€œsÃºper rÃ¡pidaâ€

```bash
make demo-lotes
```

Ejecuta fin-a-fin: init â†’ cargue inicial â†’ check â†’ validation â†’ check â†’ gold â†’ check â†’ reporte HTML.

---

## ğŸ” Reiniciar desde cero (dos opciones)

### OpciÃ³n 1 â€” Reset lÃ³gico (mantiene contenedores/volÃºmenes)

```bash
make reset-soft         # VacÃ­a RAW/SILVER/GOLD y reinicia mÃ©tricas
make db-init-schemas    # (opcional) recrea/asegura objetos
```

### OpciÃ³n 2 â€” Reset total (borra volÃºmenes Docker)

```bash
docker compose down --volumes --rmi local --remove-orphans
docker compose build --no-cache
docker compose up -d
```

---

## â–¶ï¸ Puesta en marcha tÃ­pica

```bash
# 1) Arranque + objetos
make up
make db-init-schemas

# 2) Entrenamiento (carga todo menos validation.csv), ver estado
make db-migrate        # RAWâ†’SILVER (excluye validation.csv)
make gold              # SILVERâ†’GOLD (incremental)
make check

# 3) ValidaciÃ³n (carga validation.csv) y ver cambios
make validation        # RAWâ†’SILVER (solo validation.csv)
make gold              # Incrementa GOLD con lo nuevo
make check

# 4) Reporte HTML
make report-lotes
make open-report-chrome
```

> Para detener/limpiar:

```bash
make stop   # Detiene servicios
make down   # Detiene y elimina volÃºmenes
```

---

## ğŸ§± Arquitectura del pipeline

1. **RAW** (`raw.events_raw`)  
   Ingesta directa **sin transformaciÃ³n**. Guarda texto crudo con control de duplicados por `(source_file, row_number)` e idempotencia.

2. **SILVER** (`silver.events`)  
   Normaliza y tipa:

   - `timestamp â†’ DATE` (si invÃ¡lido â†’ `1970-01-01`)
   - `price â†’ NUMERIC(18,2)` (invÃ¡lido/nulo â†’ `0.00`)
   - `user_id â†’ NUMERIC(18,0)` (invÃ¡lido/nulo â†’ `0`)  
     Marca `dq_status = 'OK' | 'COERCED'`.

3. **GOLD** (`gold.global_stats`, `gold.load_log`)  
   Mantiene **mÃ©tricas incrementales** (no reescanea histÃ³rico):
   - `total_count`, `total_sum`, `min_price`, `max_price`, `last_silver_id`
   - El **promedio** se deriva como `total_sum / total_count`.  
     **BitÃ¡cora de cargas**: `gold.load_log` registra cada **lote** (`status='BATCH'`) y los **resÃºmenes** de fase/archivo.

```
CSV (/opt/pipeline/data/raw/*.csv)
        â”‚  (validaciÃ³n de encabezado y micro-lotes)
        â–¼
RAW     raw.events_raw
        â”‚  (coerciones/validaciÃ³n â†’ SILVER por lotes)
        â–¼
SILVER  silver.events
        â”‚  (agregaciÃ³n incremental por lotes â†’ GOLD)
        â–¼
GOLD    gold.global_stats  +  gold.load_log
```

---

## ğŸ—ƒï¸ Esquema de BD (resumen)

### `raw.events_raw`

- `id BIGSERIAL PK`, `source_file TEXT`, `row_number INT` (UNIQUE con `source_file`)
- `timestamp_raw TEXT`, `price_raw TEXT`, `user_id_raw TEXT`, `loaded_at TIMESTAMPTZ`

### `silver.events`

- `raw_id BIGINT PK` (del registro en RAW)
- `event_date DATE`, `price NUMERIC(18,2)`, `user_id NUMERIC(18,0)`
- `dq_status TEXT`, `source_file TEXT`, `loaded_at TIMESTAMPTZ`

### `gold.global_stats`

- `id SMALLINT = 1`
- `total_count NUMERIC(38,0)`, `total_sum NUMERIC(38,2)`
- `min_price NUMERIC(18,2)`, `max_price NUMERIC(18,2)`
- `last_silver_id BIGINT`, `updated_at TIMESTAMPTZ`

### `gold.load_log`

- `layer TEXT` (`raw|silver|gold`)
- `file_name TEXT`
- `records BIGINT`
- `min_price NUMERIC(18,2)`, `avg_price NUMERIC(18,2)`, `max_price NUMERIC(18,2)`
- `chunk_size INT`
- `status TEXT` (`BATCH`, `SUCCESS`, `NO_NEW_ROWS`, `SKIPPED_BAD_HEADER`, â€¦)
- `details TEXT` (p.ej. `batch=3`, `file_summary`, `phase_summary`, `coerced=12`)

---

## âš™ï¸ Micro-lotes y bitÃ¡cora

- TamaÃ±o de lote por defecto: **5** (configurable con `--chunk-size` o `PIPELINE_BATCH_SIZE`).
- Las 3 fases (**RAW**, **SILVER**, **GOLD**) procesan en `LIMIT chunk_size` y registran:
  - Una fila por **lote** (`status='BATCH'`, `details='batch=N'`).
  - Una fila de **resumen** por archivo/fase (`status='SUCCESS'|...`, `details='file_summary|phase_summary'`).

**VerificaciÃ³n rÃ¡pida de lotes (SQL):**

```sql
SELECT layer, file_name, records, chunk_size, status, details
FROM gold.load_log
ORDER BY ctid;  -- orden aproximado de inserciÃ³n
```

---

## ğŸ–¥ï¸ CLI del pipeline (`app/pipeline_pg.py`)

```bash
# InicializaciÃ³n (esquemas/tablas + fila id=1 en gold.global_stats)
docker compose exec app python pipeline_pg.py init

# Orquestado (RAWâ†’SILVERâ†’GOLD) â€” por defecto exclude validation.csv si pattern=*.csv
docker compose exec app python pipeline_pg.py load \
  --data-dir /opt/pipeline/data/raw \
  --stage all \
  --chunk-size 5

# Solo RAW (sin transformaciones)
docker compose exec app python pipeline_pg.py load-raw \
  --data-dir /opt/pipeline/data/raw \
  --exclude validation.csv

# Solo RAWâ†’SILVER (entrenamiento)
docker compose exec app python pipeline_pg.py raw-to-silver --chunk-size 5

# Solo SILVERâ†’GOLD (incremental)
docker compose exec app python pipeline_pg.py silver-to-gold --chunk-size 5

# Estado rÃ¡pido
docker compose exec app python pipeline_pg.py check

# Reset lÃ³gico de datos/mÃ©tricas
docker compose exec app python pipeline_pg.py reset
```

> **Encabezado obligatorio** en CSV: `timestamp,price,user_id`.  
> Archivos sin esa cabecera se **omiten** y se registran en `gold.load_log` como `SKIPPED_BAD_HEADER`.  
> **Ruta por defecto**: `/opt/pipeline/data/raw` (`PIPELINE_DATA_DIR`).

---

## ğŸ§° Makefile (targets Ãºtiles)

- `up` â€” build & up de servicios
- `stop` / `down` â€” detener / detener + borrar volÃºmenes
- `db-init-schemas` â€” `pipeline_pg.py init`
- `db-migrate` â€” entrenamiento RAWâ†’SILVER (excluye `validation.csv`)
- `validation` â€” carga `validation.csv` RAWâ†’SILVER
- `gold` â€” agrega incrementales a GOLD
- `check` â€” estado actual
- `report-lotes` â€” **genera HTML** `app/report_lotes.html`
- `open-report-chrome` â€” abre el HTML en Google Chrome (macOS)
- `reset-soft` â€” vacÃ­a datos y reinicia mÃ©tricas
- `wipe` â€” borra volÃºmenes (reset total)
- `demo-lotes` â€” encadena todo fin-a-fin + reporte

---

## ğŸ“Š Reporte HTML de lotes y validaciÃ³n

El script `app/report_lotes.py` produce **`app/report_lotes.html`** con:

- **Cargas por lote** desde `gold.load_log`: Capa, Archivo, **Lote**, Registros, Min/Avg/Max, Chunk, Estado, Detalles.
- **SecciÃ³n VALIDATION**:
  - **Antes** (excluye `validation.csv`)
  - **Solo validation**
  - **DespuÃ©s** (total)
  - **Deltas** (DespuÃ©s â€“ Antes)

**Ejemplo:**

```bash
make report-lotes
make open-report-chrome
```

---

## ğŸ”§ ConfiguraciÃ³n y variables

- **BD (DSN)**: `DATABASE_URL` (p.ej. `postgresql://postgres:postgres@postgres:5432/postgres`)
- **Datos**: `PIPELINE_DATA_DIR` (por defecto `/opt/pipeline/data/raw`)
- **Micro-lote**: `PIPELINE_BATCH_SIZE` (por defecto `5`)
- **Jupyter**: `JUPYTER_TOKEN`
- **pgAdmin**: `PGADMIN_DEFAULT_EMAIL`, `PGADMIN_DEFAULT_PASSWORD`

> Los CSV de la prueba deben estar en `data/raw/` y respetar el encabezado requerido.

---

## ğŸ“ Estructura del repositorio

```
.
â”œâ”€â”€ Makefile
â”œâ”€â”€ README.md
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”œâ”€â”€ pipeline_pg.py
â”‚   â”œâ”€â”€ report_lotes.py
â”‚   â””â”€â”€ requirements.txt
â”œâ”€â”€ data/
â”‚   â””â”€â”€ raw/
â”‚       â”œâ”€â”€ 2012-01.csv
â”‚       â”œâ”€â”€ 2012-02.csv
â”‚       â”œâ”€â”€ 2012-03.csv
â”‚       â”œâ”€â”€ 2012-04.csv
â”‚       â”œâ”€â”€ 2012-05.csv
â”‚       â””â”€â”€ validation.csv
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ 0_START_HERE.ipynb
â””â”€â”€ pgadmin/
    â””â”€â”€ servers.json
```

---

## ğŸ› ï¸ Troubleshooting

- **Auth fallida a Postgres**  
  Asegura que `DATABASE_URL` coincide con tus `POSTGRES_*` y que el servicio `postgres` estÃ¡ arriba.

- **â€œNo se encuentra /opt/pipeline/data/rawâ€**  
  Verifica el montaje en `docker-compose.yml`:  
  `app.volumes: - ./data:/opt/pipeline/data` y que los archivos existen.

- **CSV omitido**  
  Verifica la cabecera exacta: `timestamp,price,user_id`.

- **No veo lotes en el log**  
  Ejecuta con `--chunk-size 5` y consulta:
  ```sql
  SELECT layer, file_name, records, chunk_size, status, details
  FROM gold.load_log
  ORDER BY ctid;
  ```

---

**Â¡Listo!** Con esto tienes una ejecuciÃ³n reproducible, una arquitectura clara y evidencias de micro-lotes y validaciÃ³n, todo con pocos comandos.
